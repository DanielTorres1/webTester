#!/usr/bin/python3
import argparse
import os
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import urllib3

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# List of file extensions to exclude
EXCLUDED_EXTENSIONS = ('.msi', '.frm', '.ibd', '.MYD', '.MYI', '.opt')

def download_files(url, base_folder=""):
    # Create a session to persist settings
    session = requests.Session()
    session.verify = False  # Disable SSL verification

    # Send a GET request to the URL
    response = session.get(url)
    response.raise_for_status()

    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find all links in the page
    links = soup.find_all('a')

    # Create the base folder
    base_folder = "."

    # Download each file/folder
    for link in links:
        file_url = link.get('href')
        if file_url and not file_url.startswith(('?', '/', '..')):
            full_url = urljoin(url, file_url)
            folder_name = file_url.rstrip('/')
            folder_path = os.path.join(base_folder, folder_name)

            if file_url.endswith('/'):
                # If it's a directory, create it locally
                if not os.path.exists(folder_path):
                    print(f"Creating folder {folder_path}")
                    os.makedirs(folder_path, exist_ok=True)
                # Download files in this directory
                download_directory_contents(full_url, folder_path, session)
            else:
                # If it's a file, check if it should be downloaded
                if not file_url.lower().endswith(EXCLUDED_EXTENSIONS):
                    file_path = os.path.join(base_folder, file_url)
                    print(f"Downloading file {file_path}")
                    file_response = session.get(full_url)
                    file_response.raise_for_status()
                    with open(file_path, 'wb') as f:
                        f.write(file_response.content)
                else:
                    print(f"Skipping excluded file: {file_url}")

def download_directory_contents(url, folder_path, session):
    response = session.get(url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')
    links = soup.find_all('a')

    for link in links:
        file_url = link.get('href')
        if file_url and not file_url.startswith(('?', '/', '..')):
            full_url = urljoin(url, file_url)
            file_path = os.path.join(folder_path, file_url.rstrip('/'))
            
            if file_url.endswith('/'):
                # If it's a directory, create it and process its contents
                print(f"Creating directory {file_path}")
                os.makedirs(file_path, exist_ok=True)
                download_directory_contents(full_url, file_path, session)
            else:
                # If it's a file, check if it should be downloaded
                if not file_url.lower().endswith(EXCLUDED_EXTENSIONS):
                    print(f"Downloading file {file_path}")
                    file_response = session.get(full_url)
                    file_response.raise_for_status()
                    with open(file_path, 'wb') as f:
                        f.write(file_response.content)
                else:
                    print(f"Skipping excluded file: {file_url}")

def main():
    parser = argparse.ArgumentParser(description="Download files from a web directory.")
    parser.add_argument("--url", required=True, help="URL of the web directory to download from")
    args = parser.parse_args()

    download_files(args.url)

if __name__ == "__main__":
    main()