#!/usr/bin/python3
import argparse
import os
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

EXCLUDED_EXTENSIONS = ('.msi', '.frm', '.ibd', '.MYD', '.MYI', '.opt')

def create_directory_structure(url):
    parsed_url = urlparse(url)
    path = parsed_url.path.lstrip('/')
    os.makedirs(path, exist_ok=True)
    return path

def download_files(url, session, base_folder):
    response = session.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    for link in soup.find_all('a'):
        file_url = link.get('href')
        if file_url and file_url not in ['/']:
            full_url = urljoin(url, file_url)
            relative_path = urlparse(full_url).path.lstrip('/')
            file_path = os.path.join(base_folder, relative_path)
            
            if file_url.endswith('/'):
                os.makedirs(file_path, exist_ok=True)
                download_files(full_url, session, base_folder)
            elif not file_url.lower().endswith(EXCLUDED_EXTENSIONS):
                if not os.path.exists(file_path):
                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                    print(f"Downloading: {full_url} into {file_path}")
                    with open(file_path, 'wb') as f:
                        f.write(session.get(full_url).content)
                else:
                    print(f"Skipping: {full_url} (already downloaded)")

def main():
    parser = argparse.ArgumentParser(description="Download files from a web directory.")
    parser.add_argument("--url", required=True, help="URL of the web directory")
    args = parser.parse_args()

    base_folder = create_directory_structure(args.url)
    session = requests.Session()
    session.verify = False
    download_files(args.url, session, base_folder)

if __name__ == "__main__":
    main()