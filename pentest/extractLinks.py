#!/usr/bin/python3
import re
import argparse
import os
from urllib.parse import urlparse

def extract_links(filename):
    with open(filename, 'r') as file:
        data = file.read()
        
    # Expresi√≥n regular para extraer todas las urls.    
    url_pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    urls = re.findall(url_pattern, data)
    
    # Expresiones para excluir ciertos patrones en las urls.
    exclude_patterns = [
        re.compile('\.(pdf|css|js|mp4|jpeg|png|svg|ttf|woff|woff2|eot|jpg|xml|msi)$'),
        re.compile('wp-|author|images|imagenes|contact|.org|fonts|css|google'), 
    ]

    # Filtrar urls
    for pattern in exclude_patterns:
        urls = [url for url in urls if not pattern.search(url)]
    
    # Eliminar urls repetidas.
    urls = list(set(urls))

    # Imprimir las urls.
    for url in urls:
        url=url.split('<')[0]
        print(url)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Extract urls from HTML file.')
    parser.add_argument('filename', help='The name of the HTML file to extract urls from.')

    args = parser.parse_args()

    if not os.path.exists(args.filename):
        print(f"El archivo {args.filename} no existe.")
        exit(1)
    
    extract_links(args.filename)
