#!/usr/bin/python3
import argparse
import requests
from concurrent.futures import ThreadPoolExecutor
from urllib3.exceptions import InsecureRequestWarning
import sys  # Importa el módulo sys para manejar la entrada estándar

requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)

# Número de solicitudes concurrentes
CONCURRENCY = 5

# El user agent a usar
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537"

# El proxy a usar
# PROXIES = {
#     "http": "http://127.0.0.1:8083",
#     "https": "http://127.0.0.1:8083",
# }

# El timeout de la solicitud en segundos
TIMEOUT = 10

# Códigos de estado HTTP a imprimir
STATUS_CODES = [200, 403, 404, 500]

# Lista para almacenar las URLs
urls = []

def make_request(scheme, domain):
    url = f"{scheme}://{domain}"
    try:
        response = requests.get(url, timeout=TIMEOUT, verify=False, headers={"User-Agent": USER_AGENT}, allow_redirects=True)
        if response.status_code in STATUS_CODES:
            urls.append(response.url)
    except requests.exceptions.RequestException as e:
        pass

def main(file=None):
    if file:
        with open(file, 'r') as f:
            domains = [line.strip() for line in f]
    else:
        domains = [line.strip() for line in sys.stdin]  # Lee desde la entrada estándar

    with ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:
        for domain in domains:
            for scheme in ['http', 'https']:
                executor.submit(make_request, scheme, domain)

    # Elimina duplicados e imprime
    urls_set = set(urls)
    for url in urls_set:
        print(url)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Realiza solicitudes GET a una lista de dominios.')
    parser.add_argument('-file', type=str, help='Archivo con la lista de dominios', required=False)  # Hace esta opción opcional
    args = parser.parse_args()

    main(args.file)
