#!/usr/bin/python3
import argparse
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
from concurrent.futures import ThreadPoolExecutor
from urllib3.exceptions import InsecureRequestWarning

requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)

# Number of concurrent requests
CONCURRENCY = 5

# The user agent to use
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537"

# The proxy to use
PROXIES = {
    "http": "http://127.0.0.1:8083",
    "https": "http://127.0.0.1:8083",
}

# The request timeout in seconds
TIMEOUT = 5

# HTTP status codes to print
STATUS_CODES = [200, 301, 302, 403, 404, 500]

def make_request(scheme, domain):
    url = f"{scheme}://{domain}"
    try:
        #response = requests.get(url, timeout=TIMEOUT, proxies=PROXIES, verify=False, headers={"User-Agent": USER_AGENT})
        response = requests.get(url, timeout=TIMEOUT, verify=False, headers={"User-Agent": USER_AGENT})
        if response.status_code in STATUS_CODES:
            print(url)            
    except requests.exceptions.RequestException as e:
        pass

def main(file):
    with open(file, 'r') as f:
        domains = [line.strip() for line in f]

    with ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:
        for domain in domains:
            for scheme in ['http', 'https']:
                executor.submit(make_request, scheme, domain)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Make HEAD requests to a list of domains.')
    parser.add_argument('-file', type=str, help='File with list of domains', required=True)
    args = parser.parse_args()

    main(args.file)
