#!/usr/bin/python3
import argparse
import requests
from concurrent.futures import ThreadPoolExecutor
from urllib3.exceptions import InsecureRequestWarning

requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)

# Number of concurrent requests
CONCURRENCY = 5

# The user agent to use
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537"

# The proxy to use
PROXIES = {
    "http": "http://127.0.0.1:8083",
    "https": "http://127.0.0.1:8083",
}

# The request timeout in seconds
TIMEOUT = 10

# HTTP status codes to print
STATUS_CODES = [200, 403, 404, 500]

# List to store the URLs
urls = []

def make_request(scheme, domain):
    url = f"{scheme}://{domain}"
    try:
        response = requests.get(url, timeout=TIMEOUT, verify=False, headers={"User-Agent": USER_AGENT}, allow_redirects=True)
        if response.status_code in STATUS_CODES:
            urls.append(response.url)
    except requests.exceptions.RequestException as e:
        pass

def main(file):
    with open(file, 'r') as f:
        domains = [line.strip() for line in f]

    with ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:
        for domain in domains:
            for scheme in ['http', 'https']:
                executor.submit(make_request, scheme, domain)

    # Remove duplicates and print
    urls_set = set(urls)
    for url in urls_set:
        print(url)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Make GET requests to a list of domains.')
    parser.add_argument('-file', type=str, help='File with list of domains', required=True)
    args = parser.parse_args()

    main(args.file)
